{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-base.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PRaezUC3M/cohort_selection/blob/master/CNN_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkVRZSWTfdg2",
        "colab_type": "text"
      },
      "source": [
        "This notebook is backed with github.\n",
        "https://github.com/PRaezUC3M/cohort_selection/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUhjUt2_2taj",
        "colab_type": "text"
      },
      "source": [
        "**Mount the drive folder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KX7MO6oMrqR4",
        "outputId": "cf5e688a-2f03-4346-8c4a-8c1aed79aa09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vluB44afpEHL",
        "colab_type": "text"
      },
      "source": [
        "# **Configurartion of the experiments**\n",
        "The different possible experiments have been automated. It is inspired in grid-search, and the parameters must be expressed in array-like style. You can find an example below.\n",
        "\n",
        "Every possible example will be generated from the combination of the different parameters given."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvpLzS-wneo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(1)\n",
        "tf.set_random_seed(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdJIJMn1pbCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNN Model\n",
        "oversampling = [False]\n",
        "load_embeddings = [True, False] # This defines if the embeddings are loaded from the wikipedia dataset\n",
        "num_classes = [None] # Auto\n",
        "embeddings_size = [200]\n",
        "conv_size = [[128]] \n",
        "conv_filter = [[3]]\n",
        "dropout = [0.45]\n",
        "fnn_size = [[64, 13], [None]] \n",
        "batch_size = [32]\n",
        "\n",
        "# VDCNN Model\n",
        "oversampling = [False]\n",
        "load_embeddings = [True, False] # This defines if the embeddings are loaded from the wikipedia dataset\n",
        "num_classes = [None] # Auto\n",
        "embeddings_size = [200]\n",
        "conv_size = [[64, 64, 128, 128, 256, 256]]\n",
        "conv_filter = [[10]]\n",
        "dropout = [0.45]\n",
        "fnn_size = [[256, 128, 128, 64], [None]]\n",
        "batch_size = [32]\n",
        "\n",
        "\n",
        "\n",
        "indexes = [\"oversampling\", \"load_embeddings\", \"num_classes\", \"embeddings_size\", \"conv_size\", \"conv_filter\", \"dropout\", \"fnn_size\", \"batch_size\"]\n",
        "param   = [oversampling, load_embeddings, num_classes, embeddings_size, conv_size, conv_filter, dropout, fnn_size, batch_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b65TBRSg3wep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "\n",
        "def combine_params(param, indexes):\n",
        "  combinations = list(itertools.product(*param))\n",
        "  param_combinations = [{k:v for k, v in zip(indexes, combination)}  for combination in combinations]\n",
        "  for p in param_combinations:\n",
        "    # The embeddings size must adapt to the embeddings loaded.\n",
        "    if p[\"load_embeddings\"]:\n",
        "      p[\"embeddings_size\"] = None\n",
        "  return param_combinations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-nFuYPyy8eW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network_parameters = combine_params(param, indexes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P6yvcz6sqM16"
      },
      "source": [
        "# **Load the train and test csv files**\n",
        "This functions load the data and generate the different datasets from the given csv files. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w0NdWd-SqM18",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "SST_HOME='drive/My Drive/Colab Notebooks/CohortSelection/'\n",
        "path_train=SST_HOME+'data/train/train.csv'\n",
        "path_test=SST_HOME+'data/test/test.csv'\n",
        "\n",
        "def load(path):\n",
        "  \n",
        "  df = pd.read_csv(path,header=0, delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "  categories=df.columns[2:];\n",
        "\n",
        "  idFiles = df[['IDFILE']].as_matrix().tolist()\n",
        "\n",
        "  texts = df[['TEXT']].as_matrix()\n",
        "  X = [x[0].strip() for x in texts.tolist()]\n",
        "\n",
        "  #we only keep the columns with the categories.\n",
        "  Y = df.drop(['IDFILE', 'TEXT'], axis=1).as_matrix()\n",
        "\n",
        "\n",
        "  print(path,'dataset loaded')\n",
        "  \n",
        "  return X, Y, categories, idFiles\n",
        "\n",
        "\n",
        "train_x, train_y, CATEGORIES, _ = load(path_train)\n",
        "test_x, test_y, _, IDFILES = load(path_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqzBWeLFqiAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_y[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zswqSRmTqM2A"
      },
      "source": [
        "#**Balancing the categories**\n",
        "We can see that there are some categories way more present than others in our dataset. Let's sort them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXBDCgcT8c5Y",
        "colab_type": "text"
      },
      "source": [
        "The categories are distributed in the following way\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9zxnXt38hO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns  \n",
        "\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "(ax_train, ax_test) = fig.subplots(ncols=2, nrows=1)\n",
        "g1 = sns.barplot(x=train_y.sum(axis=0), y=CATEGORIES, ax=ax_train)\n",
        "ax_train.set_xlabel('Number of records')\n",
        "g2 = sns.barplot(x=test_y.sum(axis=0), y=CATEGORIES, ax=ax_test)\n",
        "ax_test.set_xlabel('Number of records')\n",
        "g1.set_title(\"Criteria distribution on training dataset\")\n",
        "g2.set_title(\"Criteria distribution on testing dataset\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7Bfd-GpbqM2G"
      },
      "source": [
        "Now we can see the differences in a easier way, we will add weights to the different classes, based on how present they are in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wQaO6I_tqM2H",
        "colab": {}
      },
      "source": [
        "classes = np.add(sum(train_y), sum(test_y))\n",
        "total_cases = sum(classes)\n",
        "class_weight = [1 / (c/total_cases) for c in classes]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jkLOdF0BqM2M"
      },
      "source": [
        "# **Tokenize the dataset and use the word embeddings**\n",
        "\n",
        "This shows the set of different words ordered by frequency. *tokenizer.word_index*\n",
        "\n",
        "The text is tokenized and cleaned of stop-words as well as punctuation signs. Also the numbers are substituted by a token \"nmbr\"\n",
        "\n",
        "We crop the beggining of the examples because it is the date when they\n",
        "were written down.\n",
        "\n",
        "Finally, and after being tokenized, the different sentences are padded to match the maximum length.\n",
        "\n",
        "max_length == 7813 it is really a huge vector because we have to pad\n",
        "it afterwards, in order to get it into the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H9eO6C8wqM2M",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words=stopwords.words('english')\n",
        "# Define maximum word length\n",
        "MAX_WORDS = 5000\n",
        "# Defined from the dataset itself.\n",
        "DATE_LENGTH = 23"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RexMW528qM2R",
        "colab": {}
      },
      "source": [
        "# We are using this function to clean the test set\n",
        "def tokenize_clean_text(text, tokenizer=None, max_length=None, \n",
        "                        max_words=MAX_WORDS, date_length=DATE_LENGTH):\n",
        "  \"\"\"\n",
        "  This function is in charge of tokenizing the text it is given. It also cleans\n",
        "  the text from stop-words, punctuation, and gives a special token to numbers.\n",
        "  \n",
        "  :param text: The texts to tokenize in a bidimensional python array.\n",
        "  \n",
        "  :returns: The tokenized and cleaned text in a bidimensional python array.\n",
        "            The tokenizer used to preprocess the text.\n",
        "            The maximum length used for padding.\n",
        "  \"\"\"  \n",
        "  # Consider to stemm or lemmatize the text \n",
        "  \n",
        "  cropped_date_text = [sentence[date_length:] for sentence in text]\n",
        "\n",
        "  if max_length == None:\n",
        "    # We get the maximum token length by splitting by spaces\n",
        "    max_length = max([len(sentence[date_length:].split(\" \")) for sentence in cropped_date_text])\n",
        "    \n",
        "  # We remove the numbers\n",
        "  cropped_date_numbers_text = [\" \".join([word if not word.isdigit() else \"\"\n",
        "                                for word in sentence.split()])\n",
        "                               for sentence in cropped_date_text]\n",
        "  \n",
        "  # Delete stopwords as well as every word less than 3 chars.\n",
        "  cropped_date_numbers_stopw_text = [\" \".join([word if not (word in stop_words or len(word) <= 3) else \"\"\n",
        "                                      for word in sentence.split()])\n",
        "                                     for sentence in cropped_date_numbers_text]\n",
        "  \n",
        "  if tokenizer is None:\n",
        "    tokenizer = Tokenizer(num_words=MAX_WORDS) # They use 5k words too\n",
        "    tokenizer.fit_on_texts(cropped_date_numbers_stopw_text)\n",
        "  # We tokenize the sentences\n",
        "  tokenized_text = tokenizer.texts_to_sequences(cropped_date_numbers_stopw_text)\n",
        "  \n",
        "  # Now we return the padded the sequences.\n",
        "  return pad_sequences(tokenized_text, max_length), tokenizer, max_length\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4LeXd0Z0qM2U"
      },
      "source": [
        "# **Oversampling**\n",
        "\n",
        "As the dataset is very unbalanced, we must perform over-sampling (which is to increase the minority class(es)). The RandomOverSampler **class** allows us to over-sample minority classes by  picking samples at random with replacement. We need to install the package **imbalanced-learn**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AMRFPnpbqM2W",
        "colab": {}
      },
      "source": [
        "!pip3 install -U imbalanced-learn scikit-multilearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8vIcwR6xqM2Z",
        "colab": {}
      },
      "source": [
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "def oversample(train_x, train_y):\n",
        "  lp = LabelPowerset()\n",
        "  ros = RandomOverSampler(random_state=42)\n",
        "\n",
        "  # Applies the above stated multi-label (ML) to multi-class (MC) transformation.\n",
        "  yt = lp.transform(train_y)\n",
        "  \n",
        "  x_resampled, y_resampled = ros.fit_sample(train_x_token, yt)\n",
        "\n",
        "  # Inverts the ML-MC transformation to recreate the ML set\n",
        "  y_resampled = lp.inverse_transform(y_resampled)\n",
        "  y_resampled = np.asarray([i.toarray()[0] for i in y_resampled])\n",
        "  \n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  (ax_test, ax_train) = fig.subplots(ncols=2, nrows=1)\n",
        "  g1 = sns.barplot(x=train_y.sum(axis=0), y=CATEGORIES, ax=ax_test)\n",
        "  g2 = sns.barplot(x=y_resampled.sum(axis=0), y=CATEGORIES, ax=ax_train)\n",
        "  g1.set_title(\"class distribution before resampling\")\n",
        "  g2.set_title(\"class distribution in training set after resampling\")\n",
        "  \n",
        "  return x_resampled, y_resampled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aTQ39hlAqM2d"
      },
      "source": [
        "# CNN Input\n",
        "\n",
        "Here we take a maximum length of file by obtaining the accumulative density distribution of the training examples. This way we can see how many examples fall under certain threshold. Which will be fixed to 80% of the total examples. This length is 5000 words roughly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K6yAQwqgqM2d",
        "colab": {}
      },
      "source": [
        "cropped_date_text = [sentence[DATE_LENGTH:] for sentence in train_x]\n",
        "lenghts = [len(sentence[DATE_LENGTH:].split(\" \")) for sentence in cropped_date_text]\n",
        "plt.hist(lenghts, cumulative=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bNyihoeLqM2h",
        "colab": {}
      },
      "source": [
        "# Thus, we initialize the maximum word length as\n",
        "max_length = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fsqp7ukuqM2l",
        "colab": {}
      },
      "source": [
        "train_x_token, tokenizer, max_length = tokenize_clean_text(train_x, max_length=max_length)\n",
        "\n",
        "if any([p[\"oversampling\"] for p in network_parameters]):\n",
        "  train_x_oversampled, train_y_oversampled = oversample(train_x_token, train_y)\n",
        "  \n",
        "test_x_token, _, _ = tokenize_clean_text(test_x, tokenizer, max_length)\n",
        "assert len(train_x_token) == len(train_x)\n",
        "assert len(test_x_token) == len(test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t_XzzFpUqM2o"
      },
      "source": [
        "# **TensorBoard**\n",
        "TensorBoard is a great tool for DL visualization. It shows the evolution of metrics during the training phase, as well as the weights, distributions, and even the graph of the neural net. \n",
        "\n",
        "We will be using tensorboardcolab in order to run a \n",
        "TensorBoard instance. This will initialize a ngrok machine and launch TensorBoard for us to see. \n",
        "\n",
        "TensorBoard will be accesible by the url "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sbcq5gOMqM2p",
        "colab": {}
      },
      "source": [
        "# We install tensorboard colab in case we don't have it already.\n",
        "!pip install tensorboardcolab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "79ivurQUqM2t",
        "colab": {}
      },
      "source": [
        "import tensorboardcolab as tb\n",
        "\n",
        "tbc=tb.TensorBoardColab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gyam03X6qM2x"
      },
      "source": [
        "# Load the embeddings\n",
        "\n",
        "In this step, we load the word embeddings from the wikipedia, pubmed and PMC. Then we filter them adapting to the words we have in our dataset. This is, if the word appears in the pubmed we take its weights, which are set to 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SJO1m5MwqM2y",
        "colab": {}
      },
      "source": [
        "# seemingly gensim is not installed in google colab\n",
        "!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zJsmyAUOqM20",
        "colab": {}
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "PATH_W2V = \"drive/My Drive/Colab Notebooks/CohortSelection/DL/word2vect/wikipedia-pubmed-and-PMC-w2v.bin\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "81mK3tCEqM22",
        "colab": {}
      },
      "source": [
        "def load_W2V_model(path):\n",
        "    model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
        "    print(\"Loaded W2V model\")\n",
        "    return model\n",
        "  \n",
        "def generate_embedding_weigths(word_index, max_words, model):\n",
        "  embedding_matrix = np.zeros((max_words, model.vector_size), dtype=np.float32)\n",
        "  for word, i in word_index.items():\n",
        "    if i >= max_words:\n",
        "        break\n",
        "    if word in model:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = model[word]\n",
        "  return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6qPp111eqM26",
        "colab": {}
      },
      "source": [
        "if any([p[\"load_embeddings\"] for p in network_parameters]):\n",
        "  # load the model\n",
        "  if not 'model' in locals():\n",
        "    model = load_W2V_model(PATH_W2V)\n",
        "  # Generate the weights matrix\n",
        "  embedding_matrix = generate_embedding_weigths(tokenizer.word_index, MAX_WORDS, model)\n",
        "  # Set the embeddings size to the size of the embedding vector\n",
        "  for p in network_parameters:\n",
        "    if p[\"load_embeddings\"]:\n",
        "      p[\"embeddings_size\"] = model.vector_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwBfSAEepvSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network_parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "REWEkTebqM3C"
      },
      "source": [
        "#**Create a keras Embedding model**\n",
        "\n",
        "In this section we will create a Keras CNN model, compile, and train it.\n",
        "\n",
        "In this model we can decide to generate the different embeddings for the words by training the Embedding Keras layer, or simply use a pre-trained embeddings as the wikipedia ones for example.\n",
        "\n",
        "Some information about the architecture of the net is shown bellow. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MUBwiIvyqM3C",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D\n",
        "from keras.layers import MaxPooling1D, Dropout, Conv1D, Input\n",
        "from keras.layers.merge import Concatenate, add\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils.vis_utils import plot_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMK1z7KLulK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JF3OUwmJqM3F",
        "colab": {}
      },
      "source": [
        "# Generate the sequential baseline model\n",
        "model_list = []\n",
        "print(len(network_parameters))\n",
        "for param in network_parameters:\n",
        "  # oversampling, load_embeddings, num_classes, embeddings_size, conv_size, conv_filter, dropout, fnn_size, batch_size\n",
        "  model_input = Input(shape=(max_length,))\n",
        "  # Add the embeddings\n",
        "  if param[\"load_embeddings\"]:\n",
        "    z = Embedding(MAX_WORDS, param[\"embeddings_size\"], input_length=max_length, weights=[embedding_matrix], trainable=False)(model_input)\n",
        "  else:\n",
        "    z = Embedding(MAX_WORDS, param[\"embeddings_size\"], input_length=max_length)(model_input)\n",
        "    \n",
        "  z = Dropout(param[\"dropout\"])(z)\n",
        "            \n",
        "  # Add the cnn\n",
        "  conv_blocks = []\n",
        "  for filter_size in param[\"conv_filter\"]:\n",
        "    conv = None\n",
        "    for i, cnn_layer in enumerate(param[\"conv_size\"]):\n",
        "      conv = Conv1D(cnn_layer, filter_size, padding='valid', activation='relu', strides=1)(z if conv is None else conv)\n",
        "      if (i + 1) % 2 == 0:\n",
        "        conv = MaxPooling1D(pool_size=filter_size)(conv)        \n",
        "    \n",
        "    conv_block = Flatten()(conv)\n",
        "    conv_blocks.append(conv_block)         \n",
        "   \n",
        "  z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
        "            \n",
        "  # Add the fnn\n",
        "  fnn = None\n",
        "  for fnn_layer in param[\"fnn_size\"]:\n",
        "    if fnn_layer is None:\n",
        "      break\n",
        "    fnn = Dense(fnn_layer)(z if fnn is None else fnn)\n",
        "    fnn = Dropout(param[\"dropout\"])(fnn)\n",
        "  \n",
        "  if param[\"num_classes\"] is None:\n",
        "    param[\"num_classes\"] = len(train_y[0])\n",
        "    \n",
        "  fnn = Dense(param[\"num_classes\"])(z if fnn is None else fnn)\n",
        "  model_output = Activation('sigmoid')(fnn)\n",
        "\n",
        "  net_model = Model(model_input, model_output)\n",
        "  # Compile the model\n",
        "  net_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",\n",
        "                    metrics=[\"acc\"])\n",
        "\n",
        "  # Print a summary of it.\n",
        "  net_model.summary()\n",
        "  plot_model(net_model, to_file=SST_HOME+\"DL/models/\"+'model_{}.png'.format(len(model_list)))\n",
        "  model_list.append(net_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE62gMQ0e4WG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(net_model, to_file=SST_HOME+\"DL/models/\"+'model_{}.png'.format(len(model_list)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U6YqAZ5iqM3I",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "\n",
        "class TensorBoardColabCallback(TensorBoard):\n",
        "    def __init__(self, tbc=None, write_graph=True, name=None, **kwargs):\n",
        "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
        "\n",
        "        if tbc is None:\n",
        "            return\n",
        "\n",
        "        log_dir = tbc.get_graph_path()\n",
        "\n",
        "        training_log_dir = os.path.join(log_dir, 'training_{}'.format(name))\n",
        "        super(TensorBoardColabCallback, self).__init__(training_log_dir, **kwargs)\n",
        "\n",
        "        # Log the validation metrics to a separate subdirectory\n",
        "        self.val_log_dir = os.path.join(log_dir, 'validation_{}'.format(name))\n",
        "\n",
        "    def set_model(self, model):\n",
        "        # Setup writer for validation metrics\n",
        "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
        "        super(TensorBoardColabCallback, self).set_model(model)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Pop the validation logs and handle them separately with\n",
        "        # `self.val_writer`. Also rename the keys so that they can\n",
        "        # be plotted on the same figure with the training metrics\n",
        "        logs = logs or {}\n",
        "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
        "\n",
        "        for name, value in val_logs.items():\n",
        "            # print('val_logs:',epoch, name, value)\n",
        "            summary = tf.Summary()\n",
        "            summary_value = summary.value.add()\n",
        "            summary_value.simple_value = value.item()\n",
        "            summary_value.tag = name\n",
        "            self.val_writer.add_summary(summary, epoch)\n",
        "        self.val_writer.flush()\n",
        "\n",
        "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
        "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
        "        super(TensorBoardColabCallback, self).on_epoch_end(epoch, logs)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        super(TensorBoardColabCallback, self).on_train_end(logs)\n",
        "        self.val_writer.close()\n",
        "\n",
        "tb.TensorBoardColabCallback = TensorBoardColabCallback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BcYNuc7BqM3J",
        "colab": {}
      },
      "source": [
        "def define_callbacks(name):\n",
        "  # Define the callbacks\n",
        "  tbc_callback = tb.TensorBoardColabCallback(tbc, name=name)\n",
        "   \n",
        "  callbacks = [\n",
        "      ReduceLROnPlateau(),\n",
        "      EarlyStopping(patience=4),\n",
        "      tbc_callback\n",
        "  ]\n",
        "  return callbacks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ySeqkndhqM3L",
        "colab": {}
      },
      "source": [
        "# Create an array of names\n",
        "network_names = []\n",
        "for p in network_parameters:\n",
        "  name = \"oversampling_{}_load_embeddings_t_{}_num_classes_{}_embeddings_size_{}_conv_size_{}_conv_filter_{}_dropout_{}_fnn_size_{}_batch_size_{}\".format(\n",
        "      p[\"oversampling\"], p[\"load_embeddings\"], p[\"num_classes\"], p[\"embeddings_size\"],\n",
        "      p[\"conv_size\"], p[\"conv_filter\"], p[\"dropout\"], p[\"fnn_size\"], p[\"batch_size\"])\n",
        "  name = name.replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"-\")\n",
        "  network_names.append(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNX6et1k3k4P",
        "colab_type": "text"
      },
      "source": [
        "## The models are trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ocpei7QUqM3O",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "for i, net_model in enumerate(model_list):\n",
        "  # Set a name for the model based on the tweaked parameters\n",
        "  p = network_parameters[i]\n",
        "  name = network_names[i]\n",
        "  model_path = SST_HOME+\"DL/models/\" + name\n",
        "\n",
        "  # If the model exists, don't compute it again.\n",
        "  #if os.path.isfile(model_path):\n",
        "  #  continue\n",
        "    \n",
        "  print(\"\\n\\n********************************************\\n\")    \n",
        "  print(network_names[i])\n",
        "  callbacks = define_callbacks(network_names[i])\n",
        "  # Fit the model and extract its data\n",
        "  if network_parameters[i][\"oversampling\"]:\n",
        "    history = net_model.fit(train_x_oversampled, train_y_oversampled, epochs=30, batch_size=network_parameters[i][\"batch_size\"], \n",
        "                            class_weight=class_weight, callbacks=callbacks,\n",
        "                            validation_split=0.25)\n",
        "  else:\n",
        "    history = net_model.fit(train_x_token, train_y, epochs=30, batch_size=network_parameters[i][\"batch_size\"], \n",
        "                            class_weight=class_weight, callbacks=callbacks,\n",
        "                            validation_split=0.25)\n",
        "      \n",
        "  # And save the model\n",
        "  net_model.save(model_path)\n",
        "  \n",
        "# To free memory from the gpu\n",
        "from keras import backend as K\n",
        "K.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MW44vf23uRv",
        "colab_type": "text"
      },
      "source": [
        "## The models are loaded\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn5DDecUb_vW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JetxGmrtqM3W",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "predictions_list = []\n",
        "for i, net_model in enumerate(model_list):\n",
        "  print(\"\\n\\n********************************************\\n\")\n",
        "  print(network_names[i])\n",
        "  model_path = SST_HOME+\"DL/models/\" + network_names[i]\n",
        "  try:\n",
        "    net_model = load_model(model_path)\n",
        "  except ValueError:\n",
        "    print(\"The model {} was not loaded correctly\".format(name))\n",
        "    continue\n",
        "    \n",
        "  predictions = net_model.predict(test_x_token)\n",
        "  predictions = np.array([[0 if value < 0.5 else 1 for value in prediction] for prediction in predictions])\n",
        "  predictions_list.append(predictions)\n",
        "  # measuring performance on test set\n",
        "  cr=classification_report(test_y, predictions, target_names=CATEGORIES.values)\n",
        "  print(cr)\n",
        "\n",
        "  # Release memory\n",
        "  K.clear_session()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmgYwJUqG5Tv",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1cCd5xkYqM3d"
      },
      "source": [
        "#**Generating output in XML format**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UPDeSPwfqM3e",
        "colab": {}
      },
      "source": [
        "import xml.etree.ElementTree\n",
        "import os\n",
        "NOT='not met'\n",
        "MET='met'\n",
        "\n",
        "#gets a idFile, a dictionary with the predictions (category, label) and the name of the classifier used.\n",
        "def outputToXML(idFile, dictPred, classifier):\n",
        "    \n",
        "    path=SST_HOME+'data/test/xml/'+idFile+'.xml'\n",
        "    output_dir = SST_HOME+'data/output/'+classifier\n",
        "    \n",
        "    output = output_dir+'/'+idFile+'.xml'\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "      print(\"\\n\\nmaking dir\")\n",
        "      s = os.makedirs(output_dir)\n",
        "      print(output_dir)\n",
        "      print(s)      \n",
        "   \n",
        "    et = xml.etree.ElementTree.parse(path)\n",
        "\n",
        "    new_tag = xml.etree.ElementTree.SubElement(et.getroot(), 'TAGS')\n",
        "    \n",
        "    for cat in dictPred.keys():\n",
        "        element = xml.etree.ElementTree.SubElement(new_tag, cat)    \n",
        "        if dictPred[cat]==0:\n",
        "            element.attrib['met'] = NOT \n",
        "        else:\n",
        "            element.attrib['met'] = MET\n",
        "\n",
        "    et.write(output)\n",
        "\n",
        "#function for creating a dictionary with the categories with values 0 or 1\n",
        "def iniDictPred(labels, categories):\n",
        "    \n",
        "    if len(labels)!=len(categories):\n",
        "        print('Warning!!!')\n",
        "        return None\n",
        "    \n",
        "    dictPred={}\n",
        "    i=0\n",
        "    \n",
        "    for x in categories:\n",
        "        dictPred[x]=labels[i]\n",
        "        i=i+1\n",
        "    return dictPred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3Ua3D6lQqM3f",
        "colab": {}
      },
      "source": [
        "#creates the output xml files per training\n",
        "for i, predictions in enumerate(predictions_list):\n",
        "  dictionary = zip(IDFILES, predictions)\n",
        "  for obj in dictionary:\n",
        "      idFile=str(obj[0][0])\n",
        "      labels=obj[1] #gets their predictions for this file\n",
        "      dictPred=iniDictPred(labels, CATEGORIES) #creates a dictionary to join categories and labels 0,1\n",
        "      outputToXML(idFile,dictPred,\"CNN/{}\".format(network_names[i]))\n",
        "\n",
        "  print('output xml files were generated!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IiigZnfNqM3m",
        "colab": {}
      },
      "source": [
        "# Writes the output in a file for each training.\n",
        "for i, name in enumerate(network_names):\n",
        "  print(name) \n",
        "  path = \"data/output/CNN/\"\n",
        "  n = path + \"{}\".format(name)\n",
        "  os.system('cd \"{}\" && echo \"{}\" {} {}results.txt'.format(SST_HOME, name, \">\" if i == 0 else \">>\", path))\n",
        "  os.system('cd \"{}\" && python3 track1_eval.py data/test/gold {} >> {}results.txt'.format(SST_HOME, n, path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP0jddTMAF22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}