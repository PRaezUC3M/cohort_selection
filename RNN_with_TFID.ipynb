{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN with TFID.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PRaezUC3M/cohort_selection/blob/master/RNN_with_TFID.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TQR9Sn1usb-x",
        "outputId": "4dbe12b0-5a2c-47eb-b671-ad25cebf233f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GvstijeTsb-6",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(1)\n",
        "tf.set_random_seed(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vluB44afpEHL",
        "colab_type": "text"
      },
      "source": [
        "# **Configurartion of the experiments**\n",
        "The different possible experiments have been automated. It is inspired in grid-search, and the parameters must be expressed in array-like style. You can find an example below.\n",
        "\n",
        "Every possible example will be generated from the combination of the different parameters given."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdJIJMn1pbCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import LSTM, GRU\n",
        "# best\n",
        "# 200_rnn_size_64-64_dropout_0.5_fnn_size_256_batch_size_128\n",
        "# Baseline Model\n",
        "load_embeddings = [True, False] # This defines if the embeddings are loaded from the wikipedia dataset\n",
        "num_classes = [None] # Auto\n",
        "embeddings_size = [200]\n",
        "rnn_size = [[64, 64]]\n",
        "dropout = [0.5]\n",
        "fnn_size = [[256]] #[[None]]\n",
        "batch_size = [128]\n",
        "cell_type = [GRU] # GRU\n",
        "bidirectional = [False]\n",
        "indexes = [\"load_embeddings\", \"num_classes\", \"embeddings_size\", \"rnn_size\", \"cell_type\", \"bidirectional\", \"dropout\", \"fnn_size\", \"batch_size\"]\n",
        "param   = [load_embeddings, num_classes, embeddings_size, rnn_size, cell_type, bidirectional, dropout, fnn_size, batch_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b65TBRSg3wep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "\n",
        "def combine_params(param, indexes):\n",
        "  combinations = list(itertools.product(*param))\n",
        "  param_combinations = [{k:v for k, v in zip(indexes, combination)}  for combination in combinations]\n",
        "  for p in param_combinations:\n",
        "    # The embeddings size must adapt to the embeddings loaded.\n",
        "    if p[\"load_embeddings\"]:\n",
        "      p[\"embeddings_size\"] = None\n",
        "  return param_combinations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-nFuYPyy8eW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network_parameters = combine_params(param, indexes)\n",
        "network_parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-U6xgKq_sb_B"
      },
      "source": [
        "# **Load the train and test csv files**\n",
        "This functions load the data and generate the different datasets from the given csv files. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UruHey_Zsb_C",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "SST_HOME='drive/My Drive/Colab Notebooks/CohortSelection/'\n",
        "path_train=SST_HOME+'data/train/train.csv'\n",
        "path_test=SST_HOME+'data/test/test.csv'\n",
        "\n",
        "def load(path):\n",
        "  \n",
        "  df = pd.read_csv(path,header=0, delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "  categories=df.columns[2:]\n",
        "  idFiles = df[['IDFILE']].as_matrix().tolist()\n",
        "\n",
        "  texts = df[['TEXT']].as_matrix()\n",
        "  X = [x[0].strip() for x in texts.tolist()]\n",
        "\n",
        "  #we only keep the columns with the categories.\n",
        "  Y = df.drop(['IDFILE', 'TEXT'], axis=1).as_matrix()\n",
        "\n",
        "\n",
        "  print(path,'dataset loaded')\n",
        "  \n",
        "  return X, Y, categories, idFiles\n",
        "\n",
        "\n",
        "train_x, train_y, CATEGORIES, _ = load(path_train)\n",
        "test_x, test_y, _, IDFILES = load(path_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zswqSRmTqM2A"
      },
      "source": [
        "#**Balancing the categories**\n",
        "We can see that there are some categories way more present than others in our dataset. Let's sort them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXBDCgcT8c5Y",
        "colab_type": "text"
      },
      "source": [
        "The categories are distributed in the following way\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9zxnXt38hO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns  \n",
        "\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "(ax_train, ax_test) = fig.subplots(ncols=2, nrows=1)\n",
        "g1 = sns.barplot(x=train_y.sum(axis=0), y=CATEGORIES, ax=ax_train)\n",
        "ax_train.set_xlabel('Number of records')\n",
        "g2 = sns.barplot(x=test_y.sum(axis=0), y=CATEGORIES, ax=ax_test)\n",
        "ax_test.set_xlabel('Number of records')\n",
        "g1.set_title(\"Criteria distribution on training dataset\")\n",
        "g2.set_title(\"Criteria distribution on testing dataset\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7Bfd-GpbqM2G"
      },
      "source": [
        "Now we can see the differences in a easier way, we will add weights to the different classes, based on how present they are in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wQaO6I_tqM2H",
        "colab": {}
      },
      "source": [
        "classes = np.add(sum(train_y), sum(test_y))\n",
        "total_cases = sum(classes)\n",
        "class_weight = [1 / (c/total_cases) for c in classes]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pvixevcqsb_Z"
      },
      "source": [
        "# **Tokenize the dataset and use the word embeddings**\n",
        "\n",
        "This shows the set of different words ordered by frequency. *tokenizer.word_index*\n",
        "\n",
        "The text is tokenized and cleaned of stop-words as well as punctuation signs. Also the numbers are substituted by a token \"nmbr\"\n",
        "\n",
        "We crop the beggining of the examples because it is the date when they\n",
        "were written down.\n",
        "\n",
        "Finally, and after being tokenized, the different sentences are padded to match the maximum length.\n",
        "\n",
        "max_length == 7813 it is really a huge vector because we have to pad\n",
        "it afterwards, in order to get it into the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t3ntmzICsb_a",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words=stopwords.words('english')\n",
        "# Define maximum word length\n",
        "MAX_WORDS = 5000\n",
        "# Defined from the dataset itself.\n",
        "DATE_LENGTH = 23"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lV3LXEEgsb_h",
        "colab": {}
      },
      "source": [
        "# We are using this function to clean the test set\n",
        "def tokenize_clean_text(text, tokenizer=None, max_length=None, \n",
        "                        max_words=MAX_WORDS, date_length=DATE_LENGTH):\n",
        "  \"\"\"\n",
        "  This function is in charge of tokenizing the text it is given. It also cleans\n",
        "  the text from stop-words, punctuation, and gives a special token to numbers.\n",
        "  \n",
        "  :param text: The texts to tokenize in a bidimensional python array.\n",
        "  \n",
        "  :returns: The tokenized and cleaned text in a bidimensional python array.\n",
        "            The tokenizer used to preprocess the text.\n",
        "            The maximum length used for padding.\n",
        "  \"\"\"  \n",
        "  # Consider to stemm or lemmatize the text \n",
        "  \n",
        "  cropped_date_text = [sentence[date_length:] for sentence in text]\n",
        "\n",
        "  if max_length == None:\n",
        "    # We get the maximum token length by splitting by spaces\n",
        "    max_length = max([len(sentence[date_length:].split(\" \")) for sentence in cropped_date_text])\n",
        "    \n",
        "  # We remove the numbers\n",
        "  cropped_date_numbers_text = [\" \".join([word if not word.isdigit() else \"\"\n",
        "                                for word in sentence.split()])\n",
        "                               for sentence in cropped_date_text]\n",
        "  \n",
        "  # Delete stopwords as well as every word less than 3 chars.\n",
        "  cropped_date_numbers_stopw_text = [\" \".join([word if not (word in stop_words or len(word) <= 3) else \"\"\n",
        "                                      for word in sentence.split()])\n",
        "                                     for sentence in cropped_date_numbers_text]\n",
        "  \n",
        "  vec = TfidfVectorizer(max_features=MAX_WORDS)\n",
        "  tfidf_mat = vec.fit_transform(cropped_date_numbers_stopw_text).toarray()\n",
        "  tfid_words = vec.get_feature_names()\n",
        "  \n",
        "  cropped_date_numbers_stopw_tfid_text = [\" \".join([word if word in tfid_words else \"\"\n",
        "                                          for word in sentence.split()])\n",
        "                                          for sentence in cropped_date_numbers_stopw_text]\n",
        "\n",
        "  if tokenizer is None:\n",
        "    tokenizer = Tokenizer(num_words=MAX_WORDS) # They use 5k words too\n",
        "    tokenizer.fit_on_texts(cropped_date_numbers_stopw_text)\n",
        "  # We tokenize the sentences\n",
        "  tokenized_text = tokenizer.texts_to_sequences(cropped_date_numbers_stopw_text)\n",
        "  \n",
        "  # Now we return the padded the sequences.\n",
        "  return pad_sequences(tokenized_text, max_length), tokenizer, max_length\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aTQ39hlAqM2d"
      },
      "source": [
        "# RNN Input\n",
        "\n",
        "Here we take a maximum length of file by obtaining the accumulative density distribution of the training examples. This way we can see how many examples fall under certain threshold. Which will be fixed to 80% of the total examples. This length is 5000 words roughly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K6yAQwqgqM2d",
        "colab": {}
      },
      "source": [
        "cropped_date_text = [sentence[DATE_LENGTH:] for sentence in train_x]\n",
        "lenghts = [len(sentence[DATE_LENGTH:].split(\" \")) for sentence in cropped_date_text]\n",
        "plt.hist(lenghts, cumulative=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bNyihoeLqM2h",
        "colab": {}
      },
      "source": [
        "# Thus, we initialize the maximum word length as\n",
        "max_length = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fsqp7ukuqM2l",
        "colab": {}
      },
      "source": [
        "train_x_token, tokenizer, max_length = tokenize_clean_text(train_x, max_length=max_length)\n",
        "  \n",
        "test_x_token, _, _ = tokenize_clean_text(test_x, tokenizer, max_length)\n",
        "assert len(train_x_token) == len(train_x)\n",
        "assert len(test_x_token) == len(test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t_XzzFpUqM2o"
      },
      "source": [
        "# **TensorBoard**\n",
        "TensorBoard is a great tool for DL visualization. It shows the evolution of metrics during the training phase, as well as the weights, distributions, and even the graph of the neural net. \n",
        "\n",
        "We will be using tensorboardcolab in order to run a \n",
        "TensorBoard instance. This will initialize a ngrok machine and launch TensorBoard for us to see. \n",
        "\n",
        "TensorBoard will be accesible by the url "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sbcq5gOMqM2p",
        "colab": {}
      },
      "source": [
        "# We install tensorboard colab in case we don't have it already.\n",
        "!pip install tensorboardcolab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "79ivurQUqM2t",
        "colab": {}
      },
      "source": [
        "import tensorboardcolab as tb\n",
        "\n",
        "tbc=tb.TensorBoardColab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gyam03X6qM2x"
      },
      "source": [
        "# Load the embeddings\n",
        "\n",
        "In this step, we load the word embeddings from the wikipedia, pubmed and PMC. Then we filter them adapting to the words we have in our dataset. This is, if the word appears in the pubmed we take its weights, which are set to 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SJO1m5MwqM2y",
        "colab": {}
      },
      "source": [
        "# seemingly gensim is not installed in google colab\n",
        "!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zJsmyAUOqM20",
        "colab": {}
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "PATH_W2V = \"drive/My Drive/Colab Notebooks/CohortSelection/DL/word2vect/wikipedia-pubmed-and-PMC-w2v.bin\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "81mK3tCEqM22",
        "colab": {}
      },
      "source": [
        "def load_W2V_model(path):\n",
        "    model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
        "    print(\"Loaded W2V model\")\n",
        "    return model\n",
        "  \n",
        "def generate_embedding_weigths(word_index, max_words, model):\n",
        "  embedding_matrix = np.zeros((max_words, model.vector_size), dtype=np.float32)\n",
        "  for word, i in word_index.items():\n",
        "    if i >= max_words:\n",
        "        break\n",
        "    if word in model:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = model[word]\n",
        "  return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6qPp111eqM26",
        "colab": {}
      },
      "source": [
        "if not 'model' in locals():\n",
        "  model = load_W2V_model(PATH_W2V)\n",
        "# Generate the weights matrix\n",
        "embedding_matrix = generate_embedding_weigths(tokenizer.word_index, MAX_WORDS, model)\n",
        "for p in network_parameters:\n",
        "  if p[\"load_embeddings\"]:\n",
        "    p[\"embeddings_size\"] = model.vector_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOc6GfiVtFVh",
        "colab_type": "text"
      },
      "source": [
        "#**Create a keras Embedding model**\n",
        "\n",
        "In this section we will create a Keras RNN model, compile, and train it.\n",
        "\n",
        "In this model we can decide to generate the different embeddings for the words by using a pre-trained embeddings as the wikipedia ones for example.\n",
        "\n",
        "Some information about the architecture of the net is shown bellow. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVaH2QqctjPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Activation, Embedding, Dropout, Input\n",
        "from keras.layers import LSTM, GRU, Bidirectional\n",
        "\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.optimizers import RMSprop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF4E6MqmtCAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_list = []\n",
        "for net_param in network_parameters:\n",
        "  inputs = Input(name='inputs',shape=(max_length,))\n",
        "  \n",
        "  # Add the embeddings\n",
        "  if net_param[\"load_embeddings\"]:\n",
        "    z = Embedding(MAX_WORDS, net_param[\"embeddings_size\"], input_length=max_length, weights=[embedding_matrix], trainable=False)(inputs)\n",
        "  else:\n",
        "    z = Embedding(MAX_WORDS, net_param[\"embeddings_size\"], input_length=max_length)(inputs)\n",
        "    \n",
        "  for i, rsz in enumerate(net_param[\"rnn_size\"]):\n",
        "    if not net_param[\"bidirectional\"]:\n",
        "      if i < len(net_param[\"rnn_size\"]) - 1:\n",
        "        z = net_param[\"cell_type\"](rsz, return_sequences=True)(z)\n",
        "      else:\n",
        "        z = net_param[\"cell_type\"](rsz)(z)\n",
        "    else:\n",
        "      if i < len(net_param[\"rnn_size\"]) - 1:\n",
        "        z = Bidirectional(net_param[\"cell_type\"](rsz, return_sequences=True))(z)\n",
        "      else:\n",
        "        z = Bidirectional(net_param[\"cell_type\"](rsz))(z)\n",
        "    \n",
        "  for fsz in net_param[\"fnn_size\"]:\n",
        "    if fsz is None:\n",
        "      break\n",
        "    z = Dense(fsz)(z)\n",
        "    z = Dropout(net_param[\"dropout\"])(z)\n",
        "\n",
        "  z = Dense(13,name='out_layer')(z)\n",
        "  outputs = Activation('sigmoid')(z)\n",
        "\n",
        "  net_model = Model(inputs=inputs,outputs=outputs)\n",
        "\n",
        "  net_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  # Print a summary of it.\n",
        "  net_model.summary()\n",
        "  model_list.append(net_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k_1Ff-iwRbV",
        "colab_type": "text"
      },
      "source": [
        "# TB Colab Callback\n",
        "We rewrite the tensorboardcolab callbacks to create different sessions depending on the variables our trainings have. This helps to differentiate the models in tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jsRRzrFv1NH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "\n",
        "class TensorBoardColabCallback(TensorBoard):\n",
        "    def __init__(self, tbc=None, write_graph=True, name=None, **kwargs):\n",
        "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
        "\n",
        "        if tbc is None:\n",
        "            return\n",
        "\n",
        "        log_dir = tbc.get_graph_path()\n",
        "\n",
        "        training_log_dir = os.path.join(log_dir, 'training_{}'.format(name))\n",
        "        super(TensorBoardColabCallback, self).__init__(training_log_dir, **kwargs)\n",
        "\n",
        "        # Log the validation metrics to a separate subdirectory\n",
        "        self.val_log_dir = os.path.join(log_dir, 'validation_{}'.format(name))\n",
        "\n",
        "    def set_model(self, model):\n",
        "        # Setup writer for validation metrics\n",
        "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
        "        super(TensorBoardColabCallback, self).set_model(model)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Pop the validation logs and handle them separately with\n",
        "        # `self.val_writer`. Also rename the keys so that they can\n",
        "        # be plotted on the same figure with the training metrics\n",
        "        logs = logs or {}\n",
        "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
        "\n",
        "        for name, value in val_logs.items():\n",
        "            # print('val_logs:',epoch, name, value)\n",
        "            summary = tf.Summary()\n",
        "            summary_value = summary.value.add()\n",
        "            summary_value.simple_value = value.item()\n",
        "            summary_value.tag = name\n",
        "            self.val_writer.add_summary(summary, epoch)\n",
        "        self.val_writer.flush()\n",
        "\n",
        "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
        "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
        "        super(TensorBoardColabCallback, self).on_epoch_end(epoch, logs)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        super(TensorBoardColabCallback, self).on_train_end(logs)\n",
        "        self.val_writer.close()\n",
        "\n",
        "tb.TensorBoardColabCallback = TensorBoardColabCallback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd7YRWvKvXAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_callbacks(name):\n",
        "  # Define the callbacks\n",
        "  tbc_callback = tb.TensorBoardColabCallback(tbc, name=name)\n",
        "   \n",
        "  callbacks = [\n",
        "      ReduceLROnPlateau(),\n",
        "      EarlyStopping(patience=4),\n",
        "      tbc_callback\n",
        "  ]\n",
        "  return callbacks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5g5kvMmJgl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an array of names\n",
        "network_names = []\n",
        "for p in network_parameters:\n",
        "  name = \"tfid_load_embeddings_{}_num_classes_{}_embeddings_size_{}_rnn_size_{}_cell_type_{}_bidirectional_{}_dropout_{}_fnn_size_{}_batch_size_{}\".format(\n",
        "      p[\"load_embeddings\"], p[\"num_classes\"], p[\"embeddings_size\"], p[\"rnn_size\"], \n",
        "      str(p[\"cell_type\"]).split(\".\")[-1].replace(\"'\", \"\").replace(\">\", \"\"), p[\"bidirectional\"],\n",
        "      p[\"dropout\"], p[\"fnn_size\"], p[\"batch_size\"])\n",
        "  name = name.replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"-\")\n",
        "  network_names.append(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQypOjW-Jz6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "for i, net_model in enumerate(model_list):\n",
        "  # Set a name for the model based on the tweaked parameters\n",
        "  p = network_parameters[i]\n",
        "  name = network_names[i]\n",
        "  model_path = SST_HOME+\"DL/models/\" + name\n",
        "\n",
        "  # If the model exists, don't compute it again.\n",
        "  if os.path.isfile(model_path):\n",
        "    print(\"Already Trained Model: \\n{}\".format(name))\n",
        "    net_model = load_model(model_path)\n",
        "    net_model.summary()\n",
        "    continue\n",
        "    \n",
        "  print(\"\\n\\n********************************************\\n\")    \n",
        "  print(name)\n",
        "  callbacks = define_callbacks(name)\n",
        "  # Fit the model and extract its data\n",
        "  history = net_model.fit(train_x_token, train_y, epochs=30, batch_size=network_parameters[i][\"batch_size\"], \n",
        "                          class_weight=class_weight, callbacks=callbacks,\n",
        "                          validation_split=0.25)\n",
        "      \n",
        "  # And save the model\n",
        "  net_model.save(model_path)\n",
        "  \n",
        "# To free memory from the gpu\n",
        "from keras import backend as K\n",
        "K.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Lz0JL11nmX",
        "colab_type": "text"
      },
      "source": [
        "# SKLearn Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lxy8qKkwD-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "predictions_list = []\n",
        "for i, net_model in enumerate(model_list):\n",
        "  print(\"\\n\\n********************************************\\n\")\n",
        "  print(network_names[i])\n",
        "  model_path = SST_HOME+\"DL/models/\" + network_names[i]\n",
        "  try:\n",
        "    net_model = load_model(model_path)\n",
        "  except ValueError:\n",
        "    print(\"The model {} was not loaded correctly\".format(name))\n",
        "    continue\n",
        "    \n",
        "  predictions = net_model.predict(test_x_token)\n",
        "  predictions = np.array([[0 if value < 0.5 else 1 for value in prediction] for prediction in predictions])\n",
        "  predictions_list.append(predictions)\n",
        "  # measuring performance on test set\n",
        "  cr=classification_report(test_y, predictions, target_names=CATEGORIES.values)\n",
        "  print(cr)\n",
        "\n",
        "  # Release memory\n",
        "  K.clear_session()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1cCd5xkYqM3d"
      },
      "source": [
        "#**Generating output in XML format**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UPDeSPwfqM3e",
        "colab": {}
      },
      "source": [
        "import xml.etree.ElementTree\n",
        "import os\n",
        "NOT='not met'\n",
        "MET='met'\n",
        "\n",
        "#gets a idFile, a dictionary with the predictions (category, label) and the name of the classifier used.\n",
        "def outputToXML(idFile, dictPred, classifier):\n",
        "    \n",
        "    path=SST_HOME+'data/test/xml/'+idFile+'.xml'\n",
        "    output_dir = SST_HOME+'data/output/'+classifier\n",
        "    \n",
        "    output = output_dir+'/'+idFile+'.xml'\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "      print(\"\\n\\nmaking dir\")\n",
        "      s = os.makedirs(output_dir)\n",
        "      print(output_dir)\n",
        "      print(s)      \n",
        "   \n",
        "    et = xml.etree.ElementTree.parse(path)\n",
        "\n",
        "    new_tag = xml.etree.ElementTree.SubElement(et.getroot(), 'TAGS')\n",
        "    \n",
        "    for cat in dictPred.keys():\n",
        "        element = xml.etree.ElementTree.SubElement(new_tag, cat)    \n",
        "        if dictPred[cat]==0:\n",
        "            element.attrib['met'] = NOT \n",
        "        else:\n",
        "            element.attrib['met'] = MET\n",
        "\n",
        "    et.write(output)\n",
        "\n",
        "#function for creating a dictionary with the categories with values 0 or 1\n",
        "def iniDictPred(labels, categories):\n",
        "    \n",
        "    if len(labels)!=len(categories):\n",
        "        print('Warning!!!')\n",
        "        return None\n",
        "    \n",
        "    dictPred={}\n",
        "    i=0\n",
        "    \n",
        "    for x in categories:\n",
        "        dictPred[x]=labels[i]\n",
        "        i=i+1\n",
        "    return dictPred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3Ua3D6lQqM3f",
        "colab": {}
      },
      "source": [
        "#creates the output xml files per training\n",
        "\n",
        "for i, predictions in enumerate(predictions_list):\n",
        "  dictionary = zip(IDFILES, predictions)\n",
        "  for obj in dictionary:\n",
        "      idFile=str(obj[0][0])\n",
        "      labels=obj[1] #gets their predictions for this file\n",
        "      dictPred=iniDictPred(labels, CATEGORIES) #creates a dictionary to join categories and labels 0,1\n",
        "      outputToXML(idFile,dictPred,\"RNN/{}\".format(network_names[i]))\n",
        "\n",
        "  print('output xml files were generated!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IiigZnfNqM3m",
        "colab": {}
      },
      "source": [
        "import subprocess\n",
        "# Writes the output in a file for each training.\n",
        "for i, name in enumerate(network_names):\n",
        "  print(name) \n",
        "  path = \"data/output/RNN/\"\n",
        "  n = path + \"{}\".format(name)\n",
        "  os.system('cd \"{}\" && echo \"{}\" {} {}results.txt'.format(SST_HOME, name, \">\" if i == 0 else \">>\", path))\n",
        "  os.system('cd \"{}\" && python3 track1_eval.py data/test/gold {} >> {}results.txt'.format(SST_HOME, n, path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjndDmkQlNO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}